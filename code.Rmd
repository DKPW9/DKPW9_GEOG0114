---
title: "Assessment notebook"
output: html_document
date: "2023-12-27"
---

# Notes
This R Markdown contains the code for the GEOG0114 final assessment. Code is contained in chunks with annotations/notes stored separately. 

Part 1 of this markdown will look at suicide rates across England and Wales.

Part 2 of this markdown will look to constrain these by analysing the relationship between suicide and employment characteristics across England and Wales.

# Part 1: Understanding suicide rates across England and Wales
```{r Loading packages}
library(sf)
library(tmap)
library(readr)
library(janitor)
library(dplyr)
library(tidyr)
library(stringr)
library(tidyverse)
library(sf)
library(tmap)
library(nngeo)
library(spdep)
library(sp)
library(data.table)
library(HH)
library(car)
library(spatialreg)
library(spgwr)

# Set the working directory
setwd("/Users/hannah/Desktop/PSA/assessment")
```

##  Loading suicide data
Data contains raw count of number of suicies per local authority for each year. I want to extract 2021 and make a count per 100,000 people for standardisation purposes. 

Source: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/suicidesbylocalauthority

```{r Data wrangling: suicide data}
suicides2021 <- read.csv("2021suicides.csv", skip = 4, header = TRUE)

# Clean the dataset
suicides2021 <- clean_names(suicides2021)

# Extract only relevant columns and rename
suicides2021 <- suicides2021 %>%
  dplyr::select(c(1, 6)) %>%
  dplyr::rename(code = `area_code_note_2`) %>%
  dplyr::rename(suicide_count = `x2021`)
  
# Check how data is being stored
Datatypelist <- suicides2021 %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist

# Convert columns into numeric (removing character rows)
suicides2021 <- suicides2021 %>%
  mutate(suicide_count = as.numeric(suicide_count))
```

## Loading population count data
This is needed so counts per 100,000 people for each LA can be calculated (suicide dataset only contained raw counts)

Source: https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationandhouseholdestimatesenglandandwalescensus2021

```{r Data wrangling: population data}
population2021 <- read.csv("2021population.csv", skip = 6, header = TRUE)

# Clean the dataset
population2021 <- clean_names(population2021)

# Extract only relevant columns and rename
population2021 <- population2021 %>%
  dplyr::select(c(1, 3)) %>%
  dplyr::rename(code = `area_code_note_2`)

# Merge these using left join on the area code
suicides <- left_join(suicides2021, population2021, by = c("code" = "code"))

# Check how data is stored
Datatypelist <- suicides %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist

# Converting the total persons column to numeric by removing commas separating numbers
suicides$all_persons <- as.numeric(gsub(",", "", suicides$all_persons))

# Removing data pertaining to areas larger than local authority boundaries
# In the dataset these are described as either E0.... or W0....
suicides <- subset(suicides, grepl("^E0|^W0", code))

# Now I want to produce a count of suicides per 100,000 people so I can compare against areas of differing population sizes
suicides$suicide_standardised <- (suicides$suicide_count / suicides$all_persons) * 100000
```

Now I have my wrangled, tided suicide data (dependent variable). Next I want to load in my spatial data so I can join this with my attribute data to give me a spatial object containing suicide rates for 2021 across local authorities (LAs).

## Loading administrative boundaries
Source: https://geoportal.statistics.gov.uk/datasets/196d1a072aaa4882a50be333679d4f63/explore?location=54.000863%2C-3.313875%2C6.48

```{r Data wrangling: shapefile}
LAs <- st_read("LAD_MAY_2022_UK_BFE_V3.shp")

# Selecting only relevant columns
LAs <- LAs %>%
  dplyr::select(c(1, 8))

# Joining suicide data to LA boundaries
LAsuicides <- left_join(LAs, suicides, by = c("LAD22CD" = "code"))

# Renaming the LA column
LAsuicides <- LAsuicides %>%
  dplyr::rename(code = `LAD22CD`)

# Removing all Scotland and Northern Ireland data 
LAsuicides <- LAsuicides[!grepl("^S", LAsuicides$code), ]
LAsuicides <- LAsuicides[!grepl("^N", LAsuicides$code), ]

LAsuicides <- na.omit(LAsuicides)
```

My data is now spatial. I can move onto visualising it. For this, I will produce a thematic map.

```{r Thematic mapping}
# Producing a thematic map of this
UKsuicidesthematic <- tm_shape(LAsuicides) + 
  tm_borders(col = "grey", lwd = 0.07) + 
  tm_fill("suicide_standardised", 
          style = "jenks",
          palette = "PuRd",
          title = "Suicides per 100,000 people",
          na.show = TRUE,
          na.color = "white") +
  tm_compass(position = c("right", "top"), type = "arrow") + 
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(main.title = "Suicide rate in England and Wales, 2021",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.title.size = 1.5,
            legend.text.size = 1,
            legend.position = c("left", "top"))

tmap_save(UKsuicidesthematic, filename = "UKsuicidesthematic.png", width = 10, height = 8, units = "in", dpi = 300)
```

Visual inspection of this map suggests there is some clustering in suicide. To check this, I will first use Global Moran's I and test for spatial autocorrelation. 

Before I can run this, I want to exclude islands from this analysis and only look at mainland England + Wales. This is because there are additional social issues/differences that exist beyond the mainland that will not be accounted for in this study. 

```{r Data filtering}
# Creating a new column 'num_neighbors' to store the number of shared boundaries
LAsuicides$num_neighbors <- lengths(st_touches(LAsuicides, LAsuicides))

# Filtering + removing islands (those with 0 neighbours)
LAsuicides <- LAsuicides[LAsuicides$num_neighbors > 0, ]
```

I now have my filtered, final dataset ready for analysis.

## Testing for spatial autocorrelation: Global Moran's I

```{r Global Moran's I}
# Step 1: Calculating centroids of all local authorities
coordsW <- LAsuicides%>%
  st_centroid()%>%
  st_geometry()

# Plotting these
plot(coordsW,axes=TRUE)

# Step 2: Creating a weights list
# For this, I will use Queen's case (Q)
neighbours <- LAsuicides %>%
  poly2nb(., queen=T)

# Looking at summary statistics
summary(neighbours)

# Plotting neighbours
plot(neighbours, st_geometry(coordsW), col="red")
plot(LAsuicides$geometry, add=T)

# Step 3: Creating a spatial weights matrix
# For this, I will use row standardisation (W) for the weight style
LA.lw <- neighbours %>%
  nb2mat(., style="W")

# Step 4: Converting the matrix into the spatial weight list type
# This is required for Moran's I
LA.lw <- neighbours %>%
  nb2listw(., style="C")

# Step 5: Calculating Global Moran's I
moran_test <- LAsuicides %>%
  pull(suicide_standardised) %>%
  as.vector()%>%
  moran.test(., LA.lw)

moran_test
```

Interpreting these results:
Moran's I: 0.28
- This suggests a positive spatial autocorrelation, indicating that there is a tendency for similar values (suicide rates in this case) to be close to each other in space

P-value: <2.2e-16
- This very low P-value suggests that there is strong evidence to reject the null hypothesis of spatial randomness (ie. spatial  distribution of suicides is not random; there is a significant spatial pattern)

Alternative hypothesis: Greater
- Areas with high suicide rates are surrounded by other areas with high suicide rates, and areas with low suicide rates are surrounded by other areas with low suicide rates.

In summary, spatial autocorrelation of suicide rates is identified and the observed pattern is not likely due to chance.

To properly check these results, I will use a Monte Carlo simulation. This compares the score my Moran's I returned to a randomly distributed version of the variables.

```{r Monte Carlo simulation}
# Running the Monte Carlo simulation 599 times
moran_sim <- LAsuicides %>%
  pull(suicide_standardised) %>%
  as.vector() %>%
  moran.mc(., listw = LA.lw, nsim = 599)

# inspect
moran_sim
```
Interpreting these results:
This test points as to whether the relationship is significant - this test is advantageous as it breaks the relationship between polyhons and attribute value to some extent. 

P-value: 0.001667
- This is the probability of observing a Moran's I as extreme as the one calculated from actual data assuming the null hypothesis is true
- This P-value is small, indicating statistical significance
- Therefore, there's enough evidence to conclude that there's a spatial pattern in my data, with values tending to be positively autocorrelated

Whilst the Global Moran's I + Monte Carlo allows me to test for the overall measure of spatial autocorrelation in the study area, undertaking a Local Moran's I test will develop my analysis and allow me to identify specific locations where clustering/dispersion is occurring.

## Further testing for spatial autocorrelation: Local Moran's I (LISA)

```{r Local Moran's I}
# Creating an SP object
suicidesSP <- as_Spatial(LAsuicides, IDs=LAsuicides$code)

# Creating an NB object
suicides_nb <- poly2nb(suicidesSP, row.names=suicidesSP$code)

# Creating the list weights object
nb_weights_list <- nb2listw(suicides_nb, style='W')

# Running Local Moran's I
local_moran_suicides <- localmoran(suicidesSP$suicide_standardised, nb_weights_list)

# Before mapping, I need to rescale this so that the mean is 0
# I want to compare this rescaled value against neighbours
suicidesSP$rescaled_suicide <- scale(suicidesSP$suicide_standardised)

# For comparison, I need to create a spatial lag variable stored in a new column
suicidesSP$lag_scale_suicide <- lag.listw(nb_weights_list, suicidesSP$rescaled_suicide)

# Converting back to sf to prepare data for exploration of local patterns
suicide_moran_stats <- st_as_sf(suicidesSP)

# Creating labels for LISA findings
# Setting a significance value
sig_level <- 0.1

# Creating a classification with this significance value
suicide_moran_stats$quad_sig <- ifelse(suicide_moran_stats$rescaled_suicide > 0 & 
                                          suicide_moran_stats$lag_scale_suicide > 0 & 
                                          local_moran_suicides[,5] <= sig_level, 
                                          'high-high', 
                                   ifelse(suicide_moran_stats$rescaled_suicide <= 0 & 
                                          suicide_moran_stats$lag_scale_suicide <= 0 & 
                                          local_moran_suicides[,5] <= sig_level, 
                                          'low-low', 
                                   ifelse(suicide_moran_stats$rescaled_suicide > 0 & 
                                          suicide_moran_stats$lag_scale_suicide <= 0 & 
                                          local_moran_suicides[,5] <= sig_level, 
                                          'high-low', 
                                   ifelse(suicide_moran_stats$rescaled_suicide <= 0 & 
                                          suicide_moran_stats$lag_scale_suicide > 0 & 
                                          local_moran_suicides[,5] <= sig_level, 
                                          'low-high',
                                   ifelse(local_moran_suicides[,5] > sig_level, 
                                          'not-significant', 
                                          'not-significant')))))

# classification without significance value
suicide_moran_stats$quad_non_sig <- ifelse(suicide_moran_stats$rescaled_suicide > 0 & 
                                              suicide_moran_stats$lag_scale_suicide > 0, 
                                              'high-high', 
                                       ifelse(suicide_moran_stats$rescaled_suicide <= 0 & 
                                              suicide_moran_stats$lag_scale_suicide <= 0, 
                                              'low-low', 
                                       ifelse(suicide_moran_stats$rescaled_suicide > 0 & 
                                              suicide_moran_stats$lag_scale_suicide <= 0, 
                                              'high-low', 
                                       ifelse(suicide_moran_stats$rescaled_suicide <= 0 & 
                                              suicide_moran_stats$lag_scale_suicide > 0,
                                              'low-high',NA))))

# Plotting the data non-spatially
# Ideally, axes should split the data neatly into their different area vs spatial lag relationship categories
# Plot 1: results without the statistical significance
ggplot(suicide_moran_stats, aes(x = rescaled_suicide, 
                                   y = lag_scale_suicide, 
                                   color = quad_non_sig)) +
  geom_vline(xintercept = 0) + # plot vertical line
  geom_hline(yintercept = 0) + # plot horizontal line
  xlab('Scaled suicides (n)') +
  ylab('Lagged Scaled suicides (n)') +
  labs(colour='Relative to neighbours') +
  geom_point()

# Plot 2: results with the statistical significance
ggplot(suicide_moran_stats, aes(x = rescaled_suicide, 
                                   y = lag_scale_suicide, 
                                   color = quad_sig)) +
  geom_vline(xintercept = 0) + # plot vertical line
  geom_hline(yintercept = 0) + # plot horizontal line
  xlab('Scaled suicides (n)') +
  ylab('Lagged Scaled suicides (n)') +
  labs(colour='Relative to neighbours') +
  geom_point()
```

Now I want to map the results of my LISA.

```{r Mapping LISA results part 1}
# Map 1: all results 
LISA <- tm_shape(suicide_moran_stats) + 
  tm_fill(col = 'quad_non_sig', palette = c("#de2d26", "#fee0d2", "#deebf7", "#3182bd")) +
  tm_compass(position = c("right", "top"), type = "arrow") + 
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(main.title = "Local clusters of suicides, 2021 (LISA)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.title.size = 1.5,
            legend.text.size = 1,
            legend.position = c("left", "top"))

tmap_save(LISA, filename = "LISA.png", width = 10, height = 8, units = "in", dpi = 300)
```
This indicates that the clustering of local authorities with high suicide rates is located in three predominant areas. Firstly, in the north of England. Secondly, across most of Wales (bar the south). Finally, in west England, extending from roughly the boundaries of the home counties all the way to Cornwall. By contrast, lower suicide rates occur in the south of England and London. However, this is a plot of all results. Since I am mainly interested in locating areas of high suicide rates (near other LAs with high suicide rates), I will plot only the statistically significant results and see if this reveals any more nuanced geospatial variation.

```{r Mapping LISA results part 2}
# Map 2: statistically significant results
# Creating a filtered significance column for plotting
suicide_moran_stats$filtered_quad_sig <- ifelse(
  suicide_moran_stats$quad_sig %in% c("high-high", "low-low", "not-significant"),
  suicide_moran_stats$quad_sig,
  "not-significant"
)

# Renaming the column
suicide_moran_stats <- suicide_moran_stats %>%
  dplyr::rename(significant_clusters = `filtered_quad_sig`)

LISAsig <- tm_shape(suicide_moran_stats) + 
  tm_fill(col = 'significant_clusters', 
          palette = c("#de2d26", "#87b6da", "white"),
          title = "Significant clusters") +
  tm_borders(col = "grey") +
  tm_compass(position = c("right", "top"), type = "arrow") + 
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(main.title = "Significant clusters of suicides, 2021 (LISA)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.title.size = 1.5, 
            legend.text.size = 1,   
            legend.position = c("left", "top"))

LISAsig

tmap_save(LISAsig, filename = "LISAsig.png", width = 10, height = 8, units = "in", dpi = 300)
```
This map indicates that patterns of suicide are highly concentrated in the north of England, central Wales and the west of England (roughly around Bristol), with some outlier LAs located nearby. Interestingly, the east of England appears to have comparatively lower suicide rates per LA.

This concludes the data exploration part of my analysis, with the pattern and location of clustering of high suicide rates understood. The following part of analysis seeks to understand (and hopefully at least partially explain) the causes of suicide across England and Wales by analysing the relationship between suicide and employment patterns/characteristics. This shall be examined using a base linear model (multivariable linear regression), spatial error models (SEM) and geographically weighted regression (GWR).

# Part 2: Analysing the relationship between suicide rates and employment characteristics
I will be looking at the characteristics from an employment perspective for the population. This includes those who are unemployed (but able to work), unable to work and unemployed (EG. retired, students etc.) and those employed according to the number of hours they work. I will load in these datasets, wrangle them and explore the data distribution ahead of modelling.

## Loading hours worked data
```{r Data wrangling: hours worked data}
hoursworked <- read.csv("hoursworked.csv", skip = 7, header = TRUE)

# Clean the dataset
hoursworked <- clean_names(hoursworked)

# Filtering for the total population counts
hoursworked <- hoursworked %>%
  filter(subgroup == 'Total population')

# Extract only relevant columns and rename
hoursworked <- hoursworked %>%
  dplyr::select(c(1, 3, 5)) %>%
  dplyr::rename(code = `local_authority_district_code`) %>%
  dplyr::rename(total_count = `population_estimate`)

# Check how data is stored
Datatypelist <- hoursworked %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist

# Converting the total persons column to numeric by removing commas separating numbers
hoursworked$total_count <- as.numeric(gsub(",", "", hoursworked$total_count))

# I noticed there's an NA value in my dataset - I will change this to 0
hoursworked <- hoursworked %>%
  mutate(total_count = ifelse(is.na(total_count) & code == "E06000053", 0, total_count))

# Reshaping my data to convert the hours_worked column into separate columns
hoursworked <- hoursworked %>%
  group_by(hours_worked) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = hours_worked, values_from = total_count) 

hoursworked <- hoursworked %>%
  dplyr::select(c(1, 3, 4, 5, 6))
```

Now I have a tided, wrangled dataset containing the raw counts of hours worked by employed people in each local authority.

Next, I will load in the employment status of people in each local authority.

## Loading employment status data
```{r Data wrangling: employment status data}
employmentstatus <- read.csv("employmentstatus.csv", skip = 6, header = TRUE)

# Clean the dataset
employmentstatus <- clean_names(employmentstatus)

# Filtering for the total population counts
employmentstatus <- employmentstatus %>%
  filter(subgroup == 'Total population')

# Extract only relevant columns and rename
employmentstatus <- employmentstatus %>%
  dplyr::select(c(1, 3, 5)) %>%
  dplyr::rename(code = `local_authority_district_code`) %>%
  dplyr::rename(total_count = `population_estimate`) %>%
  dplyr::rename(status = `economic_activity_high_level_status`)

# Check how data is stored
Datatypelist <- hoursworked %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist

# Converting the total persons column to numeric by removing commas separating numbers
employmentstatus$total_count <- as.numeric(gsub(",", "", employmentstatus$total_count))

# Reshaping my data to convert the status column into separate columns
employmentstatus <- employmentstatus %>%
  group_by(status) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = status, values_from = total_count)

employmentstatus <- employmentstatus %>%
  dplyr::select(c(1, 3, 4, 5))
```

Now I have a tided, wrangled dataset containing the counts of people according to employment status in each local authority. 

Finally, I will include my data about the inactive population (ie. those not capable of working)

# Loading inactive population data
```{r Data wrangling: inactive population data}
inactive <- read.csv("inactivestatus.csv", skip = 6, header = TRUE)

# Clean the dataset
inactive <- clean_names(inactive)

# Keep only the total population data
inactive <- inactive %>%
  filter(subgroup == 'Total population')

# Extract only relevant columns and rename
inactive <- inactive %>%
  dplyr::select(c(1, 3, 5)) %>%
  dplyr::rename(code = `local_authority_district_code`) %>%
  dplyr::rename(total_count = `population_estimate`)

# Check how data is stored
Datatypelist <- inactive %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist

# Converting the total persons column to numeric by removing commas separating numbers
inactive$total_count <- as.numeric(gsub(",", "", inactive$total_count))

# Reshaping my data to convert the hours_worked column into separate columns
inactive <- inactive %>%
  group_by(reason_for_economic_inactivity) %>%
  mutate(id = row_number()) %>%
  pivot_wider(names_from = reason_for_economic_inactivity, values_from = total_count) 

inactive <- inactive %>%
  dplyr::select(c(1, 3, 4, 5, 6, 7))

# Renaming columns for clarity
inactive <- inactive %>%
  dplyr::rename(disabled_count = `Economically inactive, long term sick/disabled`) %>%
  dplyr::rename(carer_count = `Economically inactive, looking after family/home`) %>%
  dplyr::rename(other_count = `Economically inactive, other`) %>%
  dplyr::rename(retired_count = `Economically inactive, retired`) %>%
  dplyr::rename(student_count = `Economically inactive, student`)
```

Now I have data regarding the employment status for everyone over 16 in each local authority. Finally, I want to join these together in a single object, add the LA geometries to make the data spatial and work out percentages of each bracket so I can get a portait of the entire population legally allowed to work.

```{r Joining data together}
# Creating a new dataset that characterises the entire over 16 population
merge1 <- merge(hoursworked, employmentstatus, by = 'code', all = TRUE)
wholepopulation <- merge(merge1, inactive, by = 'code', all = TRUE)

# Dropping unnecessary columns and renaming ones for ease
wholepopulation <- wholepopulation %>%
  dplyr::select(c(1, 2, 3, 4, 5, 8, 9, 10, 11, 12, 13)) %>%
  dplyr::rename(employed_0_15_count = "0 to 15") %>%
  dplyr::rename(employed_16_30_count = "16 to 30") %>%
  dplyr::rename(employed_31_48_count = "31 to 48") %>%
  dplyr::rename(employed_49_above_count = "49 or more") %>%
  dplyr::rename(unemployed_count_count = "Unemployed") 

str(wholepopulation)

# Calculating each employment status as a percentage of the whole per LA
wholepopulation <- wholepopulation %>%
  mutate(total = rowSums(.[, c("employed_0_15_count", "employed_16_30_count", "employed_31_48_count", "employed_49_above_count", "unemployed_count_count", "disabled_count", "carer_count", "other_count", "retired_count", "student_count")], na.rm = TRUE))

# Calculate the percentage for each column and store it in a new column
wholepopulation <- wholepopulation %>%
  mutate(across(employed_0_15_count:student_count, ~. / total * 100, .names = "{.col}_percentage"))

# Finally, I want to add geometries to my dataset
# Because I did filtering on LAsuicides and not LAs, I will duplicate this object and remove unnecessary columns
# Doing this simply because the filtering takes a long time for the code to run and that saves me having to do it again
LAsfiltered <- LAsuicides %>%
  dplyr::select(c(1, 4, 5))

# Joining wholepopulation data to LA boundaries
wholepopulation <- left_join(LAsfiltered, wholepopulation, by = c("code" = "code"))

# Renaming columns for ease
wholepopulation <- wholepopulation %>%
  dplyr::rename(employed_0_15_percentage = "employed_0_15_count_percentage") %>%
  dplyr::rename(employed_16_30_percentage = "employed_16_30_count_percentage") %>%
  dplyr::rename(employed_31_48_percentage = "employed_31_48_count_percentage") %>%
  dplyr::rename(employed_49_above_percentage = "employed_49_above_count_percentage") %>%
  dplyr::rename(unemployed_percentage = "unemployed_count_count_percentage") %>%
  dplyr::rename(disabled_percentage = "disabled_count_percentage") %>%
  dplyr::rename(carer_percentage = "carer_count_percentage") %>%
  dplyr::rename(other_percentage = "other_count_percentage") %>%
  dplyr::rename(retired_percentage = "retired_count_percentage") %>%
  dplyr::rename(student_percentage = "student_count_percentage")

# I can also remove the raw count columns as these are no longer needed and are making the dataset longer than necessary
wholepopulation <- wholepopulation %>%
  dplyr::select(c(1, 2, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24))
```

Now my dataset contains both raw counts of employment/unemployment status for each LA as well as the percentage of each status alongside the LA geometry.

Next, I'm interested in plotting these as thematic maps so I can visualise any patterns in my data. As I now have percentage counts, I can use these for mapping as they're standardised across LAs of different sizes.

```{r Thematic mapping 1: Economically active population}
# Producing thematic maps of the population percentage of those capable of working by hours worked (including those voluntarily unemployed)

# Map for 0 to 15 hours worked
employed_0map <- tm_shape(wholepopulation) + 
  tm_fill("employed_0_15_percentage", 
          style = "quantile", 
          n = 7, 
          palette = "Reds",
          title = "Employed: 0-15 hrs/wk (%)")+
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 1.2,
            legend.text.size = 0.9,   
            legend.width = 0.9,
            main.title = "Employed: 0-15 hours/week (%)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.position = c("left", "top"))

tmap_save(employed_0map, filename = "employed_0map.png", width = 10, height = 8, units = "in", dpi = 300)

# Map for 16 to 30 hours worked
employed_16map <- tm_shape(wholepopulation) + 
  tm_fill("employed_16_30_percentage", 
          style = "quantile", 
          n = 7, 
          palette = "Oranges",
          title = "Employed: 16-30 hrs/wk (%)")+
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 1.2,
            legend.text.size = 0.9,   
            legend.width = 0.9,
            main.title = "Employed: 16-30 hours/week (%)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.position = c("left", "top"))

tmap_save(employed_16map, filename = "employed_16map.png", width = 10, height = 8, units = "in", dpi = 300)

# Map for 31 to 48 hours worked
employed_31map <- tm_shape(wholepopulation) + 
  tm_fill("employed_31_48_percentage", 
          style = "quantile", 
          n = 7, 
          palette = "Purples",
          title = "Employed: 31-48 hrs/wk (%)") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 1.2,
            legend.text.size = 0.9,   
            legend.width = 0.9,
            main.title = "Employed: 31-48 hours/week (%)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.position = c("left", "top"))

tmap_save(employed_31map, filename = "employed_31map.png", width = 10, height = 8, units = "in", dpi = 300)

# Map for 49 hours or more worked
employed_49map <- tm_shape(wholepopulation) + 
  tm_fill("employed_49_above_percentage", 
          style = "quantile", 
          n = 7, 
          palette = "Greens",
          title = "Employed: 49+ hrs/wk (%)") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 1.2,
            legend.text.size = 0.9,   
            legend.width = 0.9,
            main.title = "Employed: 49+ hours/week (%)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.position = c("left", "top"))

tmap_save(employed_49map, filename = "employed_49map.png", width = 10, height = 8, units = "in", dpi = 300)

# Map for unemployed but capable of working
unemployed_map <- tm_shape(wholepopulation) + 
  tm_fill("unemployed_percentage", 
          style = "quantile", 
          n = 7, 
          palette = "Blues",
          title = "Unemployed (%)") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 1.2,
            legend.text.size = 0.9,   
            legend.width = 0.9,
            main.title = "Unemployed (%)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.position = c("left", "top"))

tmap_save(unemployed_map, filename = "unemployed_map.png", width = 10, height = 8, units = "in", dpi = 300)
```
```{r Thematic mapping 2: Economically inactive population}
# Producing 4 thematic maps of the economically inactive population (unemployed but with described reason)

# Map of unemployed: disabled population 
disabled_map <- tm_shape(wholepopulation) + 
  tm_fill("disabled_percentage", 
          style = "quantile", 
          n = 7, 
          palette = "Greys",
          title = "Inactive: Disabled (%)") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 1.2,
            legend.text.size = 0.9,   
            legend.width = 0.9,
            main.title = "Inactive: disabled (%)",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.position = c("left", "top"))

tmap_save(disabled_map, filename = "disabled_map.png", width = 10, height = 8, units = "in", dpi = 300)

# Map of unemployed: caring population
plot7 <- tm_shape(wholepopulation) + 
  tm_fill("carer_percentage", style = "quantile", n = 7, palette = "Purples") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Inactive: carer (%)",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

# Map of unemployed: other population
plot8 <- tm_shape(wholepopulation) + 
  tm_fill("other_percentage", style = "quantile", n = 7, palette = "GnBu") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Inactive: other (%)",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

# Map of unemployed: retired population
plot9 <- tm_shape(wholepopulation) + 
  tm_fill("retired_percentage", style = "quantile", n = 7, palette = "BuGn") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Inactive: retired (%)",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

# Map of unemployed: student population
plot10 <- tm_shape(wholepopulation) + 
  tm_fill("student_percentage", style = "quantile", n = 7, palette = "PuRd") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Inactive: student (%)",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

plot6
plot7
plot8
plot9
plot10
```
From these maps I can only make descriptive interpretations of the spatial distribution of variables. Therefore, I need to report some formal descriptive analyses of my variables.

```{r Descriptive statistics}
summary(wholepopulation)
```
# Fitting a linear regression model
I want to address the relationship between suicide rates and employment status for the over 16 population across England and Wales. For this, I will fit a linear regression where the response variable is 'suicide' and the predictors are my 10 categories of employment status. Before I fit my model however, I want to explore my variables to check their distribution. Ideally, I want my variables to display a normal distribution.

For this, I will plot histograms of my variables.

```{r Exploratory data analysis: histograms}
# Employed: 0-15 hours
employed_0 <- ggplot(wholepopulation, aes(x = employed_0_15_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Employed: 0-15 hours/wk (%)",
       y = "Density") 

ggsave("employed_0.png", plot = employed_0, width = 8, height = 6, dpi = 300)

# Employed: 16-30 hours
employed_16 <- ggplot(wholepopulation, aes(x = employed_16_30_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Employed: 16-30 hours/wk (%)",
       y = "Density") 

ggsave("employed_16.png", plot = employed_16, width = 8, height = 6, dpi = 300)

# Employed: 31-48 hours
employed_31 <- ggplot(wholepopulation, aes(x = employed_31_48_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Employed: 31-48 hours/wk (%)",
       y = "Density") 

ggsave("employed_31.png", plot = employed_31, width = 8, height = 6, dpi = 300)

# Employed: 49+ hours
employed_49 <- ggplot(wholepopulation, aes(x = employed_49_above_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Employed: 49+ hours/wk (%)",
       y = "Density") 

ggsave("employed_49.png", plot = employed_49, width = 8, height = 6, dpi = 300)

# Unemployed (but capable) hours
unemployed <- ggplot(wholepopulation, aes(x = unemployed_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Unemployed (%)",
       y = "Density") 

ggsave("unemployed.png", plot = unemployed, width = 8, height = 6, dpi = 300)

# Unemployed: disabled
disabled <- ggplot(wholepopulation, aes(x = disabled_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.2, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Inactive: disabled (%)",
       y = "Density") 

ggsave("disabled.png", plot = disabled, width = 8, height = 6, dpi = 300)

# Unemployed: other
other <- ggplot(wholepopulation, aes(x = other_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Inactive: other (%)",
       y = "Density") 

ggsave("other.png", plot = other, width = 8, height = 6, dpi = 300)

# Unemployed: carer
carer <- ggplot(wholepopulation, aes(x = carer_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.2, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Inactive: carer (%)",
       y = "Density")

ggsave("carer.png", plot = carer, width = 8, height = 6, dpi = 300)

# Unemployed: retired
retired <- ggplot(wholepopulation, aes(x = retired_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.7, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Inactive: retired (%)",
       y = "Density")

ggsave("retired.png", plot = retired, width = 8, height = 6, dpi = 300)

# Unemployed: student
student <- ggplot(wholepopulation, aes(x = student_percentage)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.3, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Inactive: student (%)",
       y = "Density")

ggsave("student.png", plot = student, width = 8, height = 6, dpi = 300)

# Suicides
suicides <- ggplot(LAsuicides, aes(x = suicide_standardised)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Suicides (/100,000 people)",
       y = "Density")

ggsave("suicides.png", plot = suicides, width = 8, height = 6, dpi = 300)
```
By plotting the distribution of each variable, I can see that not all of them have a normal distribution. Those displaying skew include
- Employed: 49+ hrs
- Unemployed
- Inactive: other
- Inactive: disabled
- Inactive: carer
- Inactive: student

Given that skewed variables violate the assumption of a linear regression, I must deal with this. I will transform all the variables that don't have a normal distribution using log transformation

```{r Transforming skewed variables}
# Applying log transformation to all the left-skewed variables in the dataset directly
# Employed (49+)
wholepopulation <- wholepopulation %>%
  mutate(employed_49_above_log = log(employed_49_above_percentage))

# Unemployed
wholepopulation <- wholepopulation %>%
  mutate(unemployed_log = log(unemployed_percentage))

# Unemployed (disabled)
wholepopulation <- wholepopulation %>%
  mutate(disabled_log = log(disabled_percentage))

# Unemployed (other)
wholepopulation <- wholepopulation %>%
  mutate(other_log = log(other_percentage))

# Unemployed (carer)
wholepopulation <- wholepopulation %>%
  mutate(carer_log = log(carer_percentage))

# Unemployed (student)
wholepopulation <- wholepopulation %>%
  mutate(student_log = log(student_percentage))
```

Now I want to plot them again to ensure the transformation worked and variables now have a normal distribution ready for modelling.

```{r Plotting transformed variables}
# Employed: 49+ hours
employed_49log<- ggplot(wholepopulation, aes(x = employed_49_above_log)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.08, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Log transformed employed: 49+ hours/wk (%)",
       y = "Density") 

ggsave("employed_49log.png", plot = employed_49log, width = 8, height = 6, dpi = 300)

# Unemployed
unemployedlog<- ggplot(wholepopulation, aes(x = unemployed_log)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.08, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Log transformed unemployed (%)",
       y = "Density")

ggsave("unemployedlog.png", plot = unemployedlog, width = 8, height = 6, dpi = 300)

# Unemployed: disabled
disabledlog <- ggplot(wholepopulation, aes(x = disabled_log)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Log transformed unemployed: disabled (%)",
       y = "Density") 

ggsave("disabledlog.png", plot = disabledlog, width = 8, height = 6, dpi = 300)

# Unemployed: other
otherlog <- ggplot(wholepopulation, aes(x = other_log)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.03, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Log transformed unemployed: other (%)",
       y = "Density") 

ggsave("otherlog.png", plot = otherlog, width = 8, height = 6, dpi = 300)

# Unemployed: carer
carerlog <- ggplot(wholepopulation, aes(x = carer_log)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.04, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Log transformed unemployed: carer (%)",
       y = "Density")

ggsave("carerlog.png", plot = carerlog, width = 8, height = 6, dpi = 300)

# Unemployed: student
studentlog <- ggplot(wholepopulation, aes(x = student_log)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Log transformed unemployed: student (%)",
       y = "Density")

ggsave("studentlog.png", plot = studentlog, width = 8, height = 6, dpi = 300)

```
No transformations have been able to allow me to fix the skew of the inactive student population. Due to this, I will drop this variable from my analysis as it's essential that all included variables fit the normality assumption of a linear regression. 

Therefore, I shall now fit my model. This will be the base model used in this analysis.

```{r Fitting the base multivariate linear regression model}
# Building the regression model and storing the output in model1
basemodel <- lm((suicide_standardised) ~ (employed_0_15_percentage) + (employed_16_30_percentage) + (employed_31_48_percentage) + (employed_49_above_log) + (unemployed_log) + (retired_percentage) + (disabled_log) + (other_log) + (carer_log), data = wholepopulation)

# Remove scientific notation
options(scipen = 7)

# Reporting the output of the model
summary(basemodel)

# Obtaining the AIC score
aic_score <- AIC(basemodel)

# Print the AIC score
cat("AIC Score:", aic_score, "\n")

```
Now that I've fitted the model, I need to extract the residuals for fine-tuning and analysis. I will extract these and put them in a new column in the dataset

```{r Extracting model residuals}
# Storing in new column
wholepopulation$residuals <- basemodel$residuals

# Looking at descriptive statistics
summary(wholepopulation$residuals)
```

Now that I've created my model, I need to check the assumptions of a linear model.

# Assumption 1: Model residuals should be normally distributed
I want to plot a histogram of my residuals to check whether there's a normal distribution.

```{r Testing assumption 1: Normality of residual distribution}
ggplot(wholepopulation, aes(x = residuals)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Residuals",
       y = "Density")
```

The residuals display a normal distribution, meaning this assumption of my linear regression has been met.

# Assumption 2: No multicolinearity in the independent variables
Unique to multiple regression (the model being used in this analysis), it is essential to check that variables are in fact independent of one another and do not display correlation amongst each other. By having variables that aren't independent of each other introduces bias, especially if the correlation is high as this can cause improper analysis. To check for multicolinearity, I will examine the VIF for the model. Any variables with a VIF value exceeding 10 suggests that the variable should be dropped from analysis.

```{r Testing assumption 2: No multicolinearity in independent variables}
vif(basemodel)
```
Some of these VIF scores are quite high (less than 10 though); specifically unemployment, retired and disabled. I want to understand what's going on better so I'm going to plot a correlation matrix. 

```{r Testing assumption 2: Plotting correlation matrix}
library(corrr)

correlation<- wholepopulation %>%
  st_drop_geometry()%>%
  dplyr::select(suicide_standardised, employed_0_15_percentage, employed_16_30_percentage, employed_31_48_percentage, employed_49_above_log, unemployed_log, retired_percentage, disabled_log, other_log, carer_log) %>%
    correlate()

# Plot this
correlation <- rplot(correlation) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  ggtitle("Correlation matrix")

correlation

png_file_path <- "/Users/hannah/Desktop/PSA/assessment/correlation.png"
ggsave(filename = png_file_path, plot = correlation, width = 10, height = 6, units = "in", dpi = 300)
```

From this, I can see that unemployment appears to display a high positive correlation with unemployed (carer) and unemployed (other). As a result, I will drop the carer and other variables as they won't be that interesting for analysis anyway, re-run it and see if that makes a difference to the model performance.

# Re-running linear model with removal of correlated variables
```{r Fitting the second base model with correlated variables dropped}
# Building the regression model and storing the output in model1
basemodel2 <- lm((suicide_standardised) ~ (employed_0_15_percentage) + (employed_16_30_percentage) + (employed_31_48_percentage) + (employed_49_above_log) + (unemployed_log) + (disabled_log), data = wholepopulation)

# Remove scientific notation
options(scipen = 7)

# Reporting the output of the model
summary(basemodel2)

# Reporting the AIC score
aic_score <- AIC(basemodel2)

# Print the AIC score
cat("AIC Score:", aic_score, "\n")
```
Based on these results, basemodel2 has a higher F-statistic, indicating an overall improvement in explaining the variance in the response variable. However, the improvement is relatively small, and other metrics like RSE and R-squared do not show a clear improvement. Therefore, I will continue testing model assumptions.

```{r Returning to prior assumption testing for new model}
# Assumption 1
wholepopulation$residuals_base2 <- basemodel2$residuals

# Looking at descriptive statistics
summary(wholepopulation$residuals_base2)

ggplot(wholepopulation, aes(x = residuals_base2)) +
  geom_histogram(aes(y = ..density..), binwidth = 0.5, fill = "transparent", color = "black") +
  geom_density(colour = "red", size = 1, adjust = 1) +
  labs(x = "Residuals_base2",
       y = "Density")

# Assumption 2
vif(basemodel2)
```

VIF has improved for all variables and residuals continue to display normal distribution.

# Assumption 3: Homoscedasticity
This means that the model residuals exhibit constant / homogenous variance. If they don't, then hetroscedasticity is present. This is important because if residuals don't have constant variance, then parameter estimates could be wrong, as could the estimates of their significance. To check for this, the residuals will be plotted against predicted values. Ideally, I want to see a cloud of points with no apparent patterning to them.

```{r Testing assumption 3: Homoscedasticity}
library(performance)
library(patchwork)

check_model(basemodel2, check="all")

par(mfrow=c(2,2))
plot(basemodel2)
```

From this, I can see that residuals display homoscedasticity from the scale-location plot which has a roughly flat and horizontal line with randomly spaced points. Residuals are also spread along the line which is good. Also positive is that points don't deviate too much from the dotted line in the Q-Q plot, demonstrating that residuals are normally distributed.

Thus, the model residuals have passed the homoscedasticity assumption test.

# Assumption 4: Collinearity between residual points (independence of errors)
This assumption states that the residuals aren't correlated in any way. If they do, then they are understood to exhibit autocorrelation which suggests that something might be going on in the background that we have not sufficiently accounted for in our model.

```{r Testing assumption 3: Patterns of autocorrelation amongst residuals}
# Plotting the residuals to identify any patterns of spatial autocorrelation
MLRresiduals <- tm_shape(wholepopulation) + 
  tm_fill("residuals_base2", 
          style = "cont",
          midpoint = 0, 
          palette = "-RdBu",
          title = "Residuals: MLR") +
  tm_compass(position = c("right", "top"), type = "arrow") + 
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(main.title = "Multivariate linear regression base model: residuals",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.title.size = 1.5,
            legend.text.size = 1,
            legend.position = c("left", "top"))

tmap_save(MLRresiduals, filename = "MLRresiduals.png", width = 10, height = 8, units = "in", dpi = 300)
```

A visual inspection of this plot indicates that there is some spatial patterning/clustering of over-prediction (blue) and under prediction (orangey-red). This needs confirming through Moran's I test on residuals.

```{r Moran's I test on residuals}
# Generate unique number for each row
wholepopulation$ROWNUM <- 1:nrow(wholepopulation)

# We need to coerce the sf wholepopulation object into a new sp object
wholepopulation_2.0 <- as(wholepopulation, "Spatial")

# Create spatial weights matrix for areas
weights <- poly2nb(wholepopulation_2.0, row.names = wholepopulation_2.0$ROWNUM)
weightsmatrix <- nb2mat(weights, style='B')
residual_weightmatrix <- mat2listw(weightsmatrix , style='W')

# Run the test on the regression model output object "basemodel2" using lm.morantest()
lm.morantest(basemodel2, residual_weightmatrix, alternative="two.sided")
```
This has given me a positive Moran's I (0.078217870) and low P-value (0.009178), meaning that there does appear to be a significant spatial pattern in the residuals of your regression model and the null hypothesis can be rejected. In other words, this means that the residuals are somewhat related to each other and thus not independent. This means that a spatial regression would be more appropriate for modelling this type of data since theres evidence of spatial autocorrelation. 

I will next run a spatial error model. Underpinning my selection to run this kind of model is the decision to treat the identified spatial autocorrelation in residuals as something to deal with, perhaps reflecting some spatial autocorrelation amongst unobserved independent variables or some other mis-specification of the model. Basically, I want to ensure that the spatial autocorrelation is being accounted for in my model. 

My reason for not running a spatial lag model on the dependent variable is because that model assumes that dependencies exist directly among the levels of the dependent variable. Here, I don't think that suicides in one LA affect suicides in another. 

Instead, a spatial error model assumes that error terms are correlated across observations (ie. error of an observed value affects the errors of its neighbours)

# Fitting  a spatial error model
```{r Fitting the spatial error model (SEM)}
# Fitting the SEM
SEmodel <- errorsarlm((suicide_standardised) ~ (employed_0_15_percentage) + (employed_16_30_percentage) + (employed_31_48_percentage) + (employed_49_above_log) + (unemployed_log) + (disabled_log), data = wholepopulation, residual_weightmatrix)

# Reporting results of the model to see if it's more appropriate than a non-spatial one 
# Key outputs are the rho-coefficient, log-likelihood ratio test's p-value and the AIC
summary(SEmodel)

# Extracting the residuals to carry out Moran's I again to ensure the output statistic is lower than what was obtained for the Moran's I for the linear model
wholepopulation$SEresiduals <- SEmodel$residuals

# Running Moran's I test using moran.mc() function
moran.mc(wholepopulation$SEresiduals, residual_weightmatrix, 1000, zero.policy = T)
```

When comparing these results to that of basemodel2, it appears that the spatial error model is a better fit for the data based on the AIC and LR test. In terms of AIC, SE model scores better (1706.6). 

```{r Spatial error model residuals mapping }
# Plotting the residuals to identify any patterns of spatial autocorrelation
SEresiduals <- tm_shape(wholepopulation) + 
  tm_fill("SEresiduals", 
          style = "cont",
          midpoint = 0, 
          palette = "-RdBu",
          title = "Residuals: SE")+
  tm_compass(position = c("right", "top"), type = "arrow") + 
  tm_scale_bar(position = c("right", "bottom")) +
  tm_layout(main.title = "Spatial error model: residuals",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.title.size = 1.5,
            legend.text.size = 1,
            legend.position = c("left", "top"))

tmap_save(SEresiduals, filename = "SEresiduals.png", width = 10, height = 8, units = "in", dpi = 300)

```

# Geographically Weighted Regression
Finally, I am going to try fitting a Geographically Weighted Regression (GWR) model. This is used to indicate where non-stationarity may take place across space; it can be used to identify how locally weighted regression coefficients may vary across the study area. This is unlike the LM and SE models which provide global coefficients. 

GWR requires points since weights are distance-based. Therefore, centroids of the data points must be calculated and then deposited as coordinates within the spatial data frame.

```{r Fitting a Geographically Weighted Regression model}
# Calculating centroids of each LA
wholepopulation <- st_centroid(wholepopulation)

# Storing coordinates into wholepopulation. Note: longitude column is X and latitude column is Y
wholepopulation <- cbind(wholepopulation, st_coordinates(wholepopulation))

# Fitting a GWR using Adaptive Bandwidth
# This is good because the algorithm will compute/specify adaptive kernel that involves using varying bandwidth to define a region around regression points

# Finding the bandwidth
BwG <- gwr.sel((suicide_standardised) ~ (employed_0_15_percentage) + (employed_16_30_percentage) + (employed_31_48_percentage) + (employed_49_above_log) + (unemployed_log) + (disabled_log), data = wholepopulation, coords = cbind(wholepopulation$X, wholepopulation$Y), adapt = TRUE)

# See optimal bandwidth
BwG

# Fitting GWR based on this bandwidth
# Creating a timer to time how long it takes to run GWR
start.timer <- proc.time()

# Running GWR
gwr.model <- gwr(suicide_standardised~ employed_0_15_percentage + employed_16_30_percentage + employed_31_48_percentage + employed_49_above_log + unemployed_log + disabled_log, data = wholepopulation, coords = cbind(wholepopulation$X, wholepopulation$Y), adapt=BwG, hatmatrix=TRUE, se.fit=TRUE)

# End the timer and calculate how it took for model to run
end.timer <- proc.time() - start.timer

# Report the time taken
end.timer

# Print model results
gwr.model

# Since model results are stored as a SDF object in the model output, I need to extract it
gwr.data <- as.data.frame(gwr.model$SDF)

# I want to save the output as a csv to avoid having to run the model again in the future
write.csv(gwr.data, file = "gwr output.csv", row.names = FALSE)

# Running Moran's I test on residuals
moran.test(gwr.data$localR2,residual_weightmatrix, 1000, zero.policy = TRUE)
```
I will write the code to plot these results but I don't think I'll use this since my results don't look great
```{r Plotting results of GWR}
# Creating a neat spatial data frame by keeping first two columns
LAresult <- st_drop_geometry(wholepopulation[,c(1,2)])

# Inserting coefficients into lsoa_result object
LAresult$Coef0_15 <- gwr.data[,"employed_0_15_percentage"]
LAresult$Coef16_30 <- gwr.data[,"employed_16_30_percentage"]
LAresult$Coef31_48 <- gwr.data[,"employed_31_48_percentage"]
LAresult$Coef49log <- gwr.data[,"employed_49_above_log"]
LAresult$CoefUnemployed <- gwr.data[,"unemployed_log"]
LAresult$CoefLogDisabled <- gwr.data[,"disabled_log"]

# Inserting standard errors into lsoa_result object
LAresult$SE0_15 <- gwr.data[,"employed_0_15_percentage_se"]
LAresult$SE16_30 <- gwr.data[,"employed_16_30_percentage_se"]
LAresult$SE31_48 <- gwr.data[,"employed_31_48_percentage_se"]
LAresult$SE49log <- gwr.data[,"employed_49_above_log_se"]
LAresult$SEUnemployed <- gwr.data[,"unemployed_log_se"]
LAresult$SELogDisabled <- gwr.data[,"disabled_log_se"]

# Inserting localR2 estimates into lsoa_result object
LAresult$localR2 <- gwr.data[,"localR2"]

# Turning it into a spatial object so it can be mapped
LAresult <- left_join(LAresult, LAsfiltered, by = c("code" = "code"))
LAresult_sf <- st_sf(LAresult)
LAresult_sf <- LAresult_sf %>%
  dplyr::select(c(-suicide_standardised.y)) %>%
  dplyr::rename(suicide_standardised = `suicide_standardised.x`)

# Plotting coefficients
tm_shape(LAresult_sf) + 
tm_fill("Coef0_15", 
        title = "Coefficient: Employed 0-15 hr/wk [%]", 
        style = "cont", 
        midpoint = 0, 
        palette = "RdBu") +
tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "black") +
tm_compass(position = c("right", "top")) +
tm_scale_bar(position = c("left", "bottom")) +
tm_layout(frame = FALSE, 
          legend.title.size = 1, 
          legend.text.size = 1)
```

I won't include this in analysis but this was a valuable step in understanding my data.

I can also plot this map for each of my variables so as to understand the spatial patterning of relationships. However, this doesn't show me statistically significant associations. Therefore, I need to compute significance estimates by extracting a T-score.

```{r Determining coefficient significance}
# T-score statistic: 0-15
LAresult_sf$tstat0_15 <- LAresult_sf$Coef0_15 / LAresult_sf$SE0_15
LAresult_sf$significant0_15 <- cut(LAresult_sf$tstat0_15,
    breaks=c(min(LAresult_sf$tstat0_15), -2, 2, max(LAresult_sf$tstat0_15)),
    labels=c("Reduction: Significant","Not Significant", "Increase: Significant"))

# T-score statistic: 16-30
LAresult_sf$tstat16_30 <- LAresult_sf$Coef16_30 / LAresult_sf$SE16_30
LAresult_sf$significant16_30 <- cut(LAresult_sf$tstat0_15,
    breaks=c(min(LAresult_sf$tstat16_30), -2, 2, max(LAresult_sf$tstat16_30)),
    labels=c("Reduction: Significant","Not Significant", "Increase: Significant"))

# T-score statistic: 31-48
LAresult_sf$tstat31_48 <- LAresult_sf$Coef31_48 / LAresult_sf$SE31_48
LAresult_sf$significant31_48 <- cut(LAresult_sf$tstat31_48,
    breaks=c(min(LAresult_sf$tstat31_48), -2, 2, max(LAresult_sf$tstat31_48)),
    labels=c("Reduction: Significant","Not Significant", "Increase: Significant"))

# T-score statistic: 49+
LAresult_sf$tstat49 <- LAresult_sf$Coef49log / LAresult_sf$SE49log
LAresult_sf$significant_49 <- cut(LAresult_sf$tstat49,
    breaks=c(min(LAresult_sf$tstat49), -2, 2, max(LAresult_sf$tstat49)),
    labels=c("Reduction: Significant","Not Significant", "Increase: Significant"))

# T-score statistic: Unemployed
LAresult_sf$tstatunemployed <- LAresult_sf$CoefUnemployed / LAresult_sf$SEUnemployed
LAresult_sf$significant_unemployed <- cut(LAresult_sf$tstatunemployed,
    breaks=c(min(LAresult_sf$tstatunemployed), -2, 2, max(LAresult_sf$tstatunemployed)),
    labels=c("Reduction: Significant","Not Significant", "Increase: Significant"))

# T-score statistic: Disabled
LAresult_sf$tstatdisabled <- LAresult_sf$CoefLogDisabled / LAresult_sf$SELogDisabled
LAresult_sf$significant_disabled <- cut(LAresult_sf$tstatdisabled,
    breaks=c(min(LAresult_sf$tstatdisabled), -2, 2, max(LAresult_sf$tstatdisabled)),
    labels=c("Reduction: Significant","Not Significant", "Increase: Significant"))
```

Now I can report which relationships are significant/not by mapping significance categories for each variable.

```{r Mapping significance}
# Mapping significance categories
sig0_15 <- tm_shape(LAresult_sf) + 
  tm_fill("significant31_48", title = "", style = "cat", labels=c("Reduction: Significant", "Not Significant", "Increase: Significant"), palette = c("red", "white", "blue")) +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Employed: 0-15 hours/wk significance",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

sig16_30 <- tm_shape(LAresult_sf) + 
  tm_fill("significant16_30", title = "", style = "cat", labels=c("Reduction: Significant", "Not Significant", "Increase: Significant"), palette = c("red", "white", "blue")) +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Employed: 16-30 hours/wk significance",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

sig31_48 <- tm_shape(LAresult_sf) + 
  tm_fill("significant31_48", title = "", style = "cat", labels=c("Reduction: Significant", "Not Significant", "Increase: Significant"), palette = c("red", "white", "blue")) +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Employed: 31-48 hours/wk significance",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

sig49 <- tm_shape(LAresult_sf) + 
  tm_fill("significant_49", title = "", style = "cat", labels=c("Reduction: Significant", "Not Significant", "Increase: Significant"), palette = c("red", "white", "blue")) +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Employed: 49+ hours/wk significance",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

sigunemployed <- tm_shape(LAresult_sf) + 
  tm_fill("significant_unemployed", title = "", style = "cat", labels=c("Reduction: Significant", "Not Significant", "Increase: Significant"), palette = c("red", "white", "blue")) +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Unemployed significance",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))

sigdisabled <- tm_shape(LAresult_sf) + 
  tm_fill("significant_disabled", title = "", style = "cat", labels=c("Reduction: Significant", "Not Significant", "Increase: Significant"), palette = c("red", "white", "blue")) +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("right", "bottom"), size = 0.5) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Disabled significance",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "top"))
```
These aren't particularly valuable for my analysis so I won't include them.

I want to plot the local R2 values to examine model performance across my study area.

```{r Plotting GWR R2}
# Mapping localR2 to examine model performance
r2 <- tm_shape(LAresult_sf) + 
  tm_fill("localR2", 
          title = "Local R2", 
          style = "cont", 
          midpoint = 0.5, 
          palette = "Spectral") +
  tm_shape(LAsfiltered) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "grey", lwd = 0.07) +
  tm_compass(position = c("right", "top"), type = "arrow") + 
  tm_scale_bar(position = c("right", "bottom")) + 
  tm_layout(legend.title.size = 1.2,
            legend.text.size = 0.9,   
            legend.width = 0.9,
            main.title = "GWR: Local R2",
            main.title.position = "center",
            main.title.size = 1.5,
            legend.position = c("left", "top"))
  
tmap_save(r2, filename = "r2.png", width = 10, height = 8, units = "in", dpi = 300)

r2
```
Areas that are darker red signify areas where the local regression model has performed poorly in predicting suicide rates and has a lower association with employment status. From this map, we can see that broadly the model was not successful in capturing suicide rates, with the association with variables being generally poor. I will include this in my analysis as part of my model critique. 

ADDITIONAL SECTION:
I want to add a map of suicides in OECD countries to the introduction of my report for context. The following section includes the code for this.
I want to map OECD countries on the basis that these countries are most similar and thus most comparable (wealthy, stable, democratic etc.)
Source: https://data.worldbank.org/indicator/SH.STA.SUIC.P5
Source: https://public.opendatasoft.com/explore/dataset/world-administrative-boundaries/export/
```{r Suicide rate worldwide}
# Loading in my data
global_suicides <- read_csv("API_SH.STA.SUIC.P5_DS2_en_csv_v2_6300198.csv")
world_outline <- st_read("world-administrative-boundaries.shp")

# Tidy world outline
world_outline <- world_outline %>%
  dplyr::select(c(3, 9))

# Join the datasets
located_suicides <- left_join(world_outline, global_suicides, by = c("color_code" = "Country Code"))

# Extract only OECD countries
OECD_suicides <- located_suicides %>%
  filter(color_code %in% c('AUS', 'AUT', 'BEL', 'CAN', 'CHL', 'COL', 'CRI', 'CZE', 'DNK', 'EST', 'FIN', 'FRA', 'DEU', 'GRC', 'HUN', 'ISL', 'IRL', 'ISR', 'ITA', 'JPN', 'KOR', 'LVA', 'LTU', 'LUX', 'MEX', 'NLD', 'NZL', 'NOR', 'POL', 'PRT', 'SVK', 'SVN', 'ESP', 'SWE', 'CHE', 'TUR', 'GBR', 'USA'))

# Plotting
OECD_suicides_map <- tm_shape(OECD_suicides) + 
  tm_fill("2019", title = "Suicide rates (/100,000 people)", palette = "PuRd") +
  tm_shape(located_suicides) + 
  tm_polygons(alpha = 0, border.alpha = 1, border.col = "black", lwd = 0.1) +
  tm_compass(position = c("left", "top"), type = "arrow", size = 0.7) + 
  tm_scale_bar(position = c("left", "bottom"), size = 0.3) + 
  tm_layout(frame = TRUE, 
            legend.title.size = 0.7,
            legend.text.size = 0.5,   
            legend.width = 0.5,
            main.title = "Suicides per 100,000 people across OECD countries (2019)",
            main.title.position = "center",
            main.title.size = 1,
            legend.position = c("left", "bottom"))

OECD_suicides_map

tmap_save(OECD_suicides_map, filename = "OECD_suicides_map.png", width = 10, height = 8, units = "in", dpi = 300)

```
I also want to make a plot of the number of suicides in England and Wales since 2001 to supplement some information in my introduction

```{r Supplementary suicide statistics for introduction}
# Load in dataset
suicidesUK <- read.csv("numberofsuicides.csv", header = TRUE)
suicidesUK <- suicidesUK %>%
  dplyr::select(c(1, 2, 3, 4)) 

# Plotting a bar chart
UKsuicideplot <- ggplot(suicidesUK, aes(x = Year, y = suicide_count_england_and_wales)) +
  geom_line(color = "purple") +
  geom_point(color = "purple", size = 1) + 
  labs(title = "Suicides in England and Wales, 2001-2022",
       x = "Year",
       y = "Number of suicides") +
  theme_minimal() +
  ylim(4200, 6000)

ggsave("suicides_england_wales_plot.png", plot = UKsuicideplot, width = 8, height = 6, dpi = 300)

```

